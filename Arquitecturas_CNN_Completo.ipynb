{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n Comparativa de Arquitecturas Convolucionales\n",
    "\n",
    "## Objetivos:\n",
    "- Dise√±ar e implementar 2 arquitecturas CNN propias\n",
    "- Implementar Transfer Learning con arquitectura cl√°sica\n",
    "- Aplicar data augmentation y regularizaci√≥n\n",
    "- Comparar experimentalmente y reportar conclusiones\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamentos Te√≥ricos\n",
    "\n",
    "### ¬øPor qu√© CNNs y no MLPs para im√°genes?\n",
    "\n",
    "**Problemas de MLPs:**\n",
    "1. **Explosi√≥n de par√°metros**: Una imagen 32√ó32√ó3 requiere 3,072 conexiones por neurona\n",
    "2. **P√©rdida de informaci√≥n espacial**: Trata cada p√≠xel independientemente\n",
    "3. **Falta de invarianza**: No reconoce patrones desplazados\n",
    "4. **Sobreajuste masivo**: Demasiados par√°metros para entrenar\n",
    "\n",
    "**Ventajas de CNNs:**\n",
    "1. **Conectividad escasa**: Cada neurona conecta solo a regi√≥n local\n",
    "2. **Compartici√≥n de par√°metros**: Mismo filtro en toda la imagen\n",
    "3. **Jerarqu√≠a de caracter√≠sticas**: Bordes ‚Üí Texturas ‚Üí Formas ‚Üí Objetos\n",
    "\n",
    "### Principios Arquitect√≥nicos Aplicados\n",
    "\n",
    "**Patr√≥n de dise√±o**: Progressive Feature Extraction\n",
    "- Capas tempranas: Caracter√≠sticas de bajo nivel (bordes, texturas)\n",
    "- Capas medias: Patrones complejos (formas, partes de objetos)\n",
    "- Capas finales: Representaciones abstractas (objetos completos)\n",
    "\n",
    "**Trade-offs considerados:**\n",
    "- Profundidad vs Eficiencia computacional\n",
    "- Capacidad de generalizaci√≥n vs Precisi√≥n en entrenamiento\n",
    "- Tama√±o del modelo vs Tiempo de inferencia\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACI√ìN INICIAL Y IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar seeds para reproducibilidad\n",
    "# Principio: Reproducibilidad es fundamental para validaci√≥n cient√≠fica\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Carga y Preparaci√≥n de Datos\n",
    "\n",
    "### Decisi√≥n arquitect√≥nica: CIFAR-10\n",
    "- **Justificaci√≥n**: 10 clases balanceadas, im√°genes 32√ó32√ó3\n",
    "- **Trade-off**: Menor complejidad que CIFAR-100, pero suficiente para demostrar conceptos\n",
    "- **Alternativa descartada**: CIFAR-100 (mayor complejidad innecesaria para primera comparaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CARGA Y EXPLORACI√ìN DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "# Cargar CIFAR-10\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Nombres de clases para interpretabilidad\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INFORMACI√ìN DEL DATASET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de y_train: {y_train.shape}\")\n",
    "print(f\"Forma de X_test: {X_test.shape}\")\n",
    "print(f\"Forma de y_test: {y_test.shape}\")\n",
    "print(f\"Tipo de datos: {X_train.dtype}\")\n",
    "print(f\"Rango de valores: [{X_train.min()}, {X_train.max()}]\")\n",
    "print(f\"N√∫mero de clases: {len(class_names)}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Visualizar muestras del dataset\n",
    "def plot_samples(X, y, class_names, n_samples=10):\n",
    "    \"\"\"\n",
    "    Visualiza muestras aleatorias del dataset.\n",
    "    \n",
    "    Args:\n",
    "        X: Im√°genes\n",
    "        y: Etiquetas\n",
    "        class_names: Nombres de las clases\n",
    "        n_samples: N√∫mero de muestras a mostrar\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.title(f\"{class_names[y[idx][0]]}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nMuestras del conjunto de entrenamiento:\")\n",
    "plot_samples(X_train, y_train, class_names)\n",
    "\n",
    "# An√°lisis de distribuci√≥n de clases\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar([class_names[i] for i in unique], counts)\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.title('Distribuci√≥n de Clases en Conjunto de Entrenamiento')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Dataset balanceado: Todas las clases tienen {counts[0]} im√°genes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento y Normalizaci√≥n\n",
    "\n",
    "**Decisi√≥n**: Normalizaci√≥n a rango [0, 1]\n",
    "- **Justificaci√≥n**: Acelera convergencia y estabiliza gradientes\n",
    "- **Alternativa**: Estandarizaci√≥n z-score (descartada por simplicidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPROCESAMIENTO DE DATOS\n",
    "# ============================================================================\n",
    "\n",
    "# Normalizaci√≥n: Escalar p√≠xeles a rango [0, 1]\n",
    "# Principio: Los gradientes fluyen mejor con valores normalizados\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convertir etiquetas a formato categ√≥rico (one-hot encoding)\n",
    "# Necesario para categorical_crossentropy\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Crear conjunto de validaci√≥n (20% del entrenamiento)\n",
    "# Principio: Validaci√≥n independiente para detectar sobreajuste temprano\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_normalized, y_train_cat, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED,\n",
    "    stratify=y_train  # Mantener distribuci√≥n de clases\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONJUNTOS DE DATOS FINALES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Entrenamiento: {X_train_final.shape[0]} im√°genes\")\n",
    "print(f\"Validaci√≥n: {X_val.shape[0]} im√°genes\")\n",
    "print(f\"Prueba: {X_test_normalized.shape[0]} im√°genes\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Data Augmentation\n",
    "\n",
    "### Patr√≥n de dise√±o: Regularizaci√≥n por Transformaci√≥n\n",
    "\n",
    "**Justificaci√≥n:**\n",
    "- Aumenta variabilidad del dataset sin recolectar m√°s datos\n",
    "- Reduce sobreajuste al hacer el modelo invariante a transformaciones\n",
    "- Simula condiciones del mundo real (rotaciones, desplazamientos, etc.)\n",
    "\n",
    "**Trade-offs:**\n",
    "- ‚úÖ Mejor generalizaci√≥n\n",
    "- ‚ùå Mayor tiempo de entrenamiento\n",
    "- Decisi√≥n: Vale la pena para datasets peque√±os como CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA AUGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "# Configuraci√≥n de transformaciones\n",
    "# Principio: Transformaciones realistas que preservan significado sem√°ntico\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,          # Rotaci√≥n aleatoria ¬±15¬∞\n",
    "    width_shift_range=0.1,      # Desplazamiento horizontal 10%\n",
    "    height_shift_range=0.1,     # Desplazamiento vertical 10%\n",
    "    horizontal_flip=True,       # Flip horizontal (natural para estos objetos)\n",
    "    zoom_range=0.1,             # Zoom in/out 10%\n",
    "    fill_mode='nearest'         # Rellenar p√≠xeles vac√≠os\n",
    ")\n",
    "\n",
    "# No aplicar augmentation en validaci√≥n/prueba\n",
    "# Principio: Evaluar en datos sin alterar\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Visualizar efectos del data augmentation\n",
    "def visualize_augmentation(datagen, image, n_samples=5):\n",
    "    \"\"\"\n",
    "    Muestra ejemplos de im√°genes aumentadas.\n",
    "    \n",
    "    Args:\n",
    "        datagen: Generador de data augmentation\n",
    "        image: Imagen original\n",
    "        n_samples: N√∫mero de ejemplos a generar\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    # Imagen original\n",
    "    plt.subplot(1, n_samples+1, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Im√°genes aumentadas\n",
    "    image_batch = image.reshape((1,) + image.shape)\n",
    "    aug_iter = datagen.flow(image_batch, batch_size=1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        batch = next(aug_iter)\n",
    "        aug_image = batch[0]\n",
    "        plt.subplot(1, n_samples+1, i+2)\n",
    "        plt.imshow(aug_image)\n",
    "        plt.title(f'Augmented {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Ejemplos de Data Augmentation:\")\n",
    "sample_image = X_train_final[0]\n",
    "visualize_augmentation(train_datagen, sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# DEFINICI√ìN DE ARQUITECTURAS\n",
    "\n",
    "## Arquitectura 1: CNN Ligera (SimpleNet)\n",
    "\n",
    "### Principios de Dise√±o:\n",
    "- **Patr√≥n**: Progressive Downsampling\n",
    "- **Filosof√≠a**: \"Less is More\" - Eficiencia sobre complejidad\n",
    "- **Inspiraci√≥n**: VGG simplificado\n",
    "\n",
    "### Justificaci√≥n Arquitect√≥nica:\n",
    "1. **Bloques convolucionales**: 3 bloques con aumento progresivo de filtros (32‚Üí64‚Üí128)\n",
    "2. **Pooling**: MaxPooling para retener caracter√≠sticas importantes\n",
    "3. **Regularizaci√≥n**: Dropout (0.25 conv, 0.5 dense) + L2 regularization\n",
    "4. **Batch Normalization**: Acelera convergencia y estabiliza entrenamiento\n",
    "\n",
    "### Trade-offs:\n",
    "- ‚úÖ R√°pido entrenamiento\n",
    "- ‚úÖ Pocos par√°metros (~500K)\n",
    "- ‚ùå Menor capacidad expresiva\n",
    "- ‚ùå Puede sub-ajustar en datasets complejos\n",
    "\n",
    "### Escalabilidad:\n",
    "- ‚úÖ Ideal para deployment en dispositivos m√≥viles\n",
    "- ‚úÖ Inferencia r√°pida\n",
    "- ‚úÖ F√°cil mantener en producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ARQUITECTURA 1: CNN LIGERA (SimpleNet)\n",
    "# ============================================================================\n",
    "\n",
    "def create_simple_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    CNN ligera con 3 bloques convolucionales.\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Bloque 1: 2√óConv(32) + MaxPool + Dropout\n",
    "    - Bloque 2: 2√óConv(64) + MaxPool + Dropout\n",
    "    - Bloque 3: 2√óConv(128) + MaxPool + Dropout\n",
    "    - Dense: 256 + Dropout + Output\n",
    "    \n",
    "    Par√°metros estimados: ~500K\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='SimpleNet')\n",
    "    \n",
    "    # ====== BLOQUE 1: Extracci√≥n de caracter√≠sticas b√°sicas ======\n",
    "    # Detecta bordes y texturas simples\n",
    "    model.add(layers.Conv2D(\n",
    "        32, (3, 3), \n",
    "        activation='relu', \n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(0.001),  # L2 regularization\n",
    "        input_shape=input_shape,\n",
    "        name='conv1_1'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn1_1'))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        32, (3, 3), \n",
    "        activation='relu', \n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='conv1_2'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn1_2'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool1'))  # 32√ó32 ‚Üí 16√ó16\n",
    "    model.add(layers.Dropout(0.25, name='dropout1'))      # Regularizaci√≥n\n",
    "    \n",
    "    # ====== BLOQUE 2: Caracter√≠sticas de nivel medio ======\n",
    "    # Detecta formas y patrones m√°s complejos\n",
    "    model.add(layers.Conv2D(\n",
    "        64, (3, 3), \n",
    "        activation='relu', \n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='conv2_1'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn2_1'))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        64, (3, 3), \n",
    "        activation='relu', \n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='conv2_2'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn2_2'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool2'))  # 16√ó16 ‚Üí 8√ó8\n",
    "    model.add(layers.Dropout(0.25, name='dropout2'))\n",
    "    \n",
    "    # ====== BLOQUE 3: Caracter√≠sticas de alto nivel ======\n",
    "    # Detecta partes de objetos y composiciones\n",
    "    model.add(layers.Conv2D(\n",
    "        128, (3, 3), \n",
    "        activation='relu', \n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='conv3_1'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn3_1'))\n",
    "    \n",
    "    model.add(layers.Conv2D(\n",
    "        128, (3, 3), \n",
    "        activation='relu', \n",
    "        padding='same',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='conv3_2'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn3_2'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool3'))  # 8√ó8 ‚Üí 4√ó4\n",
    "    model.add(layers.Dropout(0.25, name='dropout3'))\n",
    "    \n",
    "    # ====== CAPAS DENSAS: Clasificaci√≥n ======\n",
    "    model.add(layers.Flatten(name='flatten'))\n",
    "    \n",
    "    model.add(layers.Dense(\n",
    "        256, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.001),\n",
    "        name='dense1'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn_dense'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout_dense'))\n",
    "    \n",
    "    # Capa de salida con softmax para clasificaci√≥n multiclase\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model_simple = create_simple_cnn()\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"=\"*70)\n",
    "print(\"ARQUITECTURA 1: SIMPLENET\")\n",
    "print(\"=\"*70)\n",
    "model_simple.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura 2: CNN Profunda (DeepNet)\n",
    "\n",
    "### Principios de Dise√±o:\n",
    "- **Patr√≥n**: Residual-inspired (sin skip connections para simplicidad)\n",
    "- **Filosof√≠a**: Mayor profundidad = Mayor capacidad expresiva\n",
    "- **Inspiraci√≥n**: ResNet + DenseNet\n",
    "\n",
    "### Justificaci√≥n Arquitect√≥nica:\n",
    "1. **4 bloques convolucionales**: Progresi√≥n 64‚Üí128‚Üí256‚Üí512\n",
    "2. **Convoluciones 3√ó3**: Filtros peque√±os pero profundos (filosof√≠a VGG)\n",
    "3. **Batch Normalization**: Despu√©s de cada convoluci√≥n\n",
    "4. **Global Average Pooling**: Reduce overfitting vs Flatten tradicional\n",
    "5. **Regularizaci√≥n agresiva**: Dropout + L2 + BatchNorm\n",
    "\n",
    "### Trade-offs:\n",
    "- ‚úÖ Mayor capacidad representacional\n",
    "- ‚úÖ Mejor accuracy potencial\n",
    "- ‚ùå M√°s par√°metros (~2M)\n",
    "- ‚ùå Mayor riesgo de overfitting\n",
    "- ‚ùå Entrenamiento m√°s lento\n",
    "\n",
    "### Validaci√≥n contra est√°ndares:\n",
    "- Similar a arquitecturas ganadoras de ImageNet\n",
    "- Sigue principios de VGG (profundidad) y ResNet (normalizaci√≥n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ARQUITECTURA 2: CNN PROFUNDA (DeepNet)\n",
    "# ============================================================================\n",
    "\n",
    "def create_deep_cnn(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    CNN profunda con 4 bloques convolucionales y m√°s filtros.\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Bloque 1: 3√óConv(64) + MaxPool + Dropout\n",
    "    - Bloque 2: 3√óConv(128) + MaxPool + Dropout\n",
    "    - Bloque 3: 3√óConv(256) + MaxPool + Dropout\n",
    "    - Bloque 4: 3√óConv(512) + GlobalAvgPool\n",
    "    - Dense: 512 + 256 + Output\n",
    "    \n",
    "    Par√°metros estimados: ~2M\n",
    "    \"\"\"\n",
    "    model = models.Sequential(name='DeepNet')\n",
    "    \n",
    "    # ====== BLOQUE 1: Caracter√≠sticas b√°sicas ======\n",
    "    for i in range(3):\n",
    "        model.add(layers.Conv2D(\n",
    "            64, (3, 3),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.0001),\n",
    "            input_shape=input_shape if i == 0 else None,\n",
    "            name=f'conv1_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization(name=f'bn1_{i+1}'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool1'))\n",
    "    model.add(layers.Dropout(0.2, name='dropout1'))\n",
    "    \n",
    "    # ====== BLOQUE 2: Caracter√≠sticas medias ======\n",
    "    for i in range(3):\n",
    "        model.add(layers.Conv2D(\n",
    "            128, (3, 3),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.0001),\n",
    "            name=f'conv2_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization(name=f'bn2_{i+1}'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool2'))\n",
    "    model.add(layers.Dropout(0.3, name='dropout2'))\n",
    "    \n",
    "    # ====== BLOQUE 3: Caracter√≠sticas altas ======\n",
    "    for i in range(3):\n",
    "        model.add(layers.Conv2D(\n",
    "            256, (3, 3),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.0001),\n",
    "            name=f'conv3_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization(name=f'bn3_{i+1}'))\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2), name='pool3'))\n",
    "    model.add(layers.Dropout(0.4, name='dropout3'))\n",
    "    \n",
    "    # ====== BLOQUE 4: Caracter√≠sticas abstractas ======\n",
    "    for i in range(3):\n",
    "        model.add(layers.Conv2D(\n",
    "            512, (3, 3),\n",
    "            activation='relu',\n",
    "            padding='same',\n",
    "            kernel_regularizer=regularizers.l2(0.0001),\n",
    "            name=f'conv4_{i+1}'\n",
    "        ))\n",
    "        model.add(layers.BatchNormalization(name=f'bn4_{i+1}'))\n",
    "    \n",
    "    # Global Average Pooling en lugar de Flatten\n",
    "    # Reduce par√°metros y overfitting\n",
    "    model.add(layers.GlobalAveragePooling2D(name='global_avg_pool'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout4'))\n",
    "    \n",
    "    # ====== CAPAS DENSAS ======\n",
    "    model.add(layers.Dense(\n",
    "        512,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.0001),\n",
    "        name='dense1'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn_dense1'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout_dense1'))\n",
    "    \n",
    "    model.add(layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(0.0001),\n",
    "        name='dense2'\n",
    "    ))\n",
    "    model.add(layers.BatchNormalization(name='bn_dense2'))\n",
    "    model.add(layers.Dropout(0.5, name='dropout_dense2'))\n",
    "    \n",
    "    # Salida\n",
    "    model.add(layers.Dense(num_classes, activation='softmax', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model_deep = create_deep_cnn()\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"=\"*70)\n",
    "print(\"ARQUITECTURA 2: DEEPNET\")\n",
    "print(\"=\"*70)\n",
    "model_deep.summary()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura 3: Transfer Learning con MobileNetV2\n",
    "\n",
    "### Decisi√≥n arquitect√≥nica: ¬øPor qu√© MobileNetV2?\n",
    "\n",
    "**Alternativas consideradas:**\n",
    "1. **VGG16**: Descartada (demasiado pesada: 138M par√°metros)\n",
    "2. **ResNet50**: Descartada (overkill para CIFAR: 25M par√°metros)\n",
    "3. **MobileNetV2**: ‚úÖ SELECCIONADA\n",
    "\n",
    "### Justificaci√≥n:\n",
    "- **Eficiencia**: Solo 3.5M par√°metros\n",
    "- **Rendimiento**: SOTA en dispositivos m√≥viles\n",
    "- **Arquitectura**: Inverted Residuals + Linear Bottlenecks\n",
    "- **Pre-entrenamiento**: ImageNet (1.4M im√°genes)\n",
    "\n",
    "### Estrategia de Transfer Learning:\n",
    "1. **Freeze base**: Congelar capas convolucionales pre-entrenadas\n",
    "2. **Custom head**: A√±adir clasificador espec√≠fico para CIFAR-10\n",
    "3. **Fine-tuning (opcional)**: Descongelar √∫ltimas capas si es necesario\n",
    "\n",
    "### Trade-offs:\n",
    "- ‚úÖ Convergencia r√°pida (knowledge pre-aprendido)\n",
    "- ‚úÖ Mejor generalizaci√≥n\n",
    "- ‚úÖ Menos datos necesarios\n",
    "- ‚ùå Menos customizable\n",
    "- ‚ùå Overhead de memoria inicial\n",
    "\n",
    "### Escalabilidad:\n",
    "- ‚úÖ Deployment-ready para producci√≥n\n",
    "- ‚úÖ Mobile-friendly\n",
    "- ‚úÖ Inferencia eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ARQUITECTURA 3: TRANSFER LEARNING (MobileNetV2)\n",
    "# ============================================================================\n",
    "\n",
    "def create_transfer_learning_model(input_shape=(32, 32, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Modelo de Transfer Learning usando MobileNetV2 pre-entrenado.\n",
    "    \n",
    "    Estrategia:\n",
    "    1. Cargar MobileNetV2 sin top (sin capa de clasificaci√≥n)\n",
    "    2. Congelar pesos pre-entrenados\n",
    "    3. A√±adir cabeza personalizada para CIFAR-10\n",
    "    \n",
    "    Nota: MobileNetV2 espera input m√≠nimo de 32√ó32, perfecto para CIFAR\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cargar MobileNetV2 pre-entrenado en ImageNet\n",
    "    # include_top=False: Sin capa de clasificaci√≥n\n",
    "    # weights='imagenet': Usar pesos pre-entrenados\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # IMPORTANTE: Congelar las capas base\n",
    "    # Principio: Preservar conocimiento aprendido en ImageNet\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Construir modelo completo\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        \n",
    "        # Cabeza personalizada para CIFAR-10\n",
    "        layers.GlobalAveragePooling2D(name='gap'),\n",
    "        \n",
    "        # Primera capa densa con regularizaci√≥n\n",
    "        layers.Dense(\n",
    "            512,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(0.0001),\n",
    "            name='fc1'\n",
    "        ),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.Dropout(0.5, name='dropout1'),\n",
    "        \n",
    "        # Segunda capa densa\n",
    "        layers.Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(0.0001),\n",
    "            name='fc2'\n",
    "        ),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.Dropout(0.3, name='dropout2'),\n",
    "        \n",
    "        # Capa de salida\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ], name='MobileNetV2_Transfer')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model_transfer = create_transfer_learning_model()\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"=\"*70)\n",
    "print(\"ARQUITECTURA 3: TRANSFER LEARNING (MobileNetV2)\")\n",
    "print(\"=\"*70)\n",
    "model_transfer.summary()\n",
    "\n",
    "# Mostrar informaci√≥n sobre capas congeladas\n",
    "total_layers = len(model_transfer.layers)\n",
    "trainable_layers = sum([1 for layer in model_transfer.layers if layer.trainable])\n",
    "frozen_layers = total_layers - trainable_layers\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Total de capas: {total_layers}\")\n",
    "print(f\"Capas entrenables: {trainable_layers}\")\n",
    "print(f\"Capas congeladas: {frozen_layers}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ENTRENAMIENTO DE MODELOS\n",
    "\n",
    "## Configuraci√≥n de Hiperpar√°metros\n",
    "\n",
    "### Decisiones de optimizaci√≥n:\n",
    "\n",
    "**Optimizer: Adam**\n",
    "- Justificaci√≥n: Adaptive learning rate, robusto y confiable\n",
    "- Alternativas descartadas: SGD (requiere m√°s tuning), RMSprop (menos usado)\n",
    "\n",
    "**Learning Rate: 0.001 (default)**\n",
    "- Con ReduceLROnPlateau para ajuste din√°mico\n",
    "- Se reduce cuando val_loss se estanca\n",
    "\n",
    "**Loss: Categorical Crossentropy**\n",
    "- Est√°ndar para clasificaci√≥n multiclase\n",
    "\n",
    "**Callbacks:**\n",
    "1. **EarlyStopping**: Prevenir overfitting\n",
    "2. **ReduceLROnPlateau**: Ajuste adaptativo de LR\n",
    "3. **ModelCheckpoint**: Guardar mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURACI√ìN DE ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "\n",
    "# Hiperpar√°metros\n",
    "BATCH_SIZE = 64        # Balance entre velocidad y estabilidad\n",
    "EPOCHS = 50            # Suficiente con early stopping\n",
    "LEARNING_RATE = 0.001  # Default de Adam\n",
    "\n",
    "# Callbacks compartidos para todos los modelos\n",
    "def get_callbacks(model_name):\n",
    "    \"\"\"\n",
    "    Retorna callbacks configurados para entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Nombre del modelo (para checkpoint)\n",
    "    \"\"\"\n",
    "    return [\n",
    "        # Early Stopping: Detener si no hay mejora\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,          # Esperar 10 √©pocas\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce Learning Rate: Ajustar LR si se estanca\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,           # Reducir a la mitad\n",
    "            patience=5,           # Despu√©s de 5 √©pocas sin mejora\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Model Checkpoint: Guardar mejor modelo\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'best_{model_name}.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Funci√≥n de compilaci√≥n unificada\n",
    "def compile_model(model, learning_rate=LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Compila modelo con configuraci√≥n est√°ndar.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a compilar\n",
    "        learning_rate: Tasa de aprendizaje\n",
    "    \"\"\"\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"‚úì Configuraci√≥n de entrenamiento lista\")\n",
    "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  - Epochs m√°ximos: {EPOCHS}\")\n",
    "print(f\"  - Learning Rate inicial: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento Modelo 1: SimpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENTRENAR MODELO 1: SIMPLENET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENANDO SIMPLENET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compilar\n",
    "model_simple = compile_model(model_simple)\n",
    "\n",
    "# Entrenar con data augmentation\n",
    "history_simple = model_simple.fit(\n",
    "    train_datagen.flow(X_train_final, y_train_final, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=get_callbacks('simplenet'),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì SimpleNet entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento Modelo 2: DeepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENTRENAR MODELO 2: DEEPNET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENANDO DEEPNET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compilar\n",
    "model_deep = compile_model(model_deep)\n",
    "\n",
    "# Entrenar con data augmentation\n",
    "history_deep = model_deep.fit(\n",
    "    train_datagen.flow(X_train_final, y_train_final, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=get_callbacks('deepnet'),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì DeepNet entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento Modelo 3: Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENTRENAR MODELO 3: TRANSFER LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENTRENANDO TRANSFER LEARNING (MobileNetV2)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compilar\n",
    "model_transfer = compile_model(model_transfer)\n",
    "\n",
    "# Entrenar con data augmentation\n",
    "# Nota: Transfer learning generalmente converge m√°s r√°pido\n",
    "history_transfer = model_transfer.fit(\n",
    "    train_datagen.flow(X_train_final, y_train_final, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=get_callbacks('transfer'),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Transfer Learning entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EVALUACI√ìN Y COMPARACI√ìN\n",
    "\n",
    "## M√©tricas de Evaluaci√≥n\n",
    "\n",
    "Evaluaremos los modelos usando:\n",
    "1. **Accuracy**: Precisi√≥n general\n",
    "2. **Loss**: Funci√≥n de p√©rdida\n",
    "3. **Curvas de aprendizaje**: Train vs Validation\n",
    "4. **Matriz de confusi√≥n**: Errores por clase\n",
    "5. **Classification Report**: Precision, Recall, F1-Score\n",
    "6. **Tiempo de entrenamiento**: Eficiencia computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUACI√ìN EN CONJUNTO DE PRUEBA\n",
    "# ============================================================================\n",
    "\n",
    "# Evaluar todos los modelos\n",
    "models = {\n",
    "    'SimpleNet': model_simple,\n",
    "    'DeepNet': model_deep,\n",
    "    'Transfer Learning': model_transfer\n",
    "}\n",
    "\n",
    "histories = {\n",
    "    'SimpleNet': history_simple,\n",
    "    'DeepNet': history_deep,\n",
    "    'Transfer Learning': history_transfer\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUACI√ìN EN CONJUNTO DE PRUEBA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluando {name}...\")\n",
    "    \n",
    "    # Evaluar\n",
    "    test_loss, test_acc = model.evaluate(X_test_normalized, y_test_cat, verbose=0)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_normalized, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    results[name] = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'y_pred': y_pred_classes,\n",
    "        'history': histories[name]\n",
    "    }\n",
    "    \n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GR√ÅFICAS DE CURVAS DE APRENDIZAJE\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(histories, metric='accuracy'):\n",
    "    \"\"\"\n",
    "    Grafica curvas de aprendizaje para todos los modelos.\n",
    "    \n",
    "    Args:\n",
    "        histories: Diccionario con historiales de entrenamiento\n",
    "        metric: M√©trica a graficar ('accuracy' o 'loss')\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Subplot 1: Training metric\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history.history[metric], label=f'{name} Train', linewidth=2)\n",
    "    plt.title(f'Training {metric.capitalize()}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Validation metric\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'{name} Val', linewidth=2)\n",
    "    plt.title(f'Validation {metric.capitalize()}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Graficar accuracy\n",
    "print(\"Curvas de Accuracy:\")\n",
    "plot_training_history(histories, 'accuracy')\n",
    "\n",
    "# Graficar loss\n",
    "print(\"\\nCurvas de Loss:\")\n",
    "plot_training_history(histories, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPARACI√ìN DE M√âTRICAS FINALES\n",
    "# ============================================================================\n",
    "\n",
    "def plot_model_comparison(results):\n",
    "    \"\"\"\n",
    "    Gr√°fico de barras comparando m√©tricas finales.\n",
    "    \"\"\"\n",
    "    model_names = list(results.keys())\n",
    "    test_accs = [results[name]['test_acc'] for name in model_names]\n",
    "    test_losses = [results[name]['test_loss'] for name in model_names]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    bars1 = ax1.bar(model_names, test_accs, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # A√±adir valores sobre barras\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Loss\n",
    "    bars2 = ax2.bar(model_names, test_losses, color=colors, alpha=0.8, edgecolor='black')\n",
    "    ax2.set_ylabel('Loss', fontsize=12)\n",
    "    ax2.set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # A√±adir valores sobre barras\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_model_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MATRICES DE CONFUSI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "def plot_confusion_matrices(results, class_names):\n",
    "    \"\"\"\n",
    "    Grafica matrices de confusi√≥n para todos los modelos.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, (name, result) in enumerate(results.items()):\n",
    "        cm = confusion_matrix(y_test, result['y_pred'])\n",
    "        \n",
    "        # Normalizar\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names,\n",
    "                    ax=axes[idx], cbar=True)\n",
    "        axes[idx].set_title(f'{name}\\nAccuracy: {result[\"test_acc\"]:.4f}',\n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel('True Label')\n",
    "        axes[idx].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Matrices de Confusi√≥n (Normalizadas):\")\n",
    "plot_confusion_matrices(results, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# REPORTES DE CLASIFICACI√ìN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REPORTES DE CLASIFICACI√ìN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(classification_report(y_test, result['y_pred'], \n",
    "                                target_names=class_names,\n",
    "                                digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TABLA COMPARATIVA FINAL\n",
    "# ============================================================================\n",
    "\n",
    "# Crear DataFrame con m√©tricas\n",
    "comparison_data = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Contar par√°metros\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    \n",
    "    # √âpocas entrenadas\n",
    "    epochs_trained = len(histories[name].history['loss'])\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Modelo': name,\n",
    "        'Test Accuracy': f\"{results[name]['test_acc']:.4f}\",\n",
    "        'Test Loss': f\"{results[name]['test_loss']:.4f}\",\n",
    "        'Total Params': f\"{total_params:,}\",\n",
    "        'Trainable Params': f\"{trainable_params:,}\",\n",
    "        'Epochs': epochs_trained\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLA COMPARATIVA FINAL\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CONCLUSIONES\n",
    "\n",
    "## An√°lisis Comparativo de Arquitecturas\n",
    "\n",
    "### 1. SimpleNet (CNN Ligera)\n",
    "\n",
    "**Fortalezas:**\n",
    "- ‚úÖ **Eficiencia computacional**: Entrenamiento r√°pido y menor uso de memoria\n",
    "- ‚úÖ **Deployability**: Ideal para dispositivos con recursos limitados\n",
    "- ‚úÖ **Interpretabilidad**: Arquitectura simple y f√°cil de debuggear\n",
    "\n",
    "**Debilidades:**\n",
    "- ‚ùå **Capacidad limitada**: Puede sub-ajustar en datasets complejos\n",
    "- ‚ùå **Representaci√≥n**: Menos profundidad = menos jerarqu√≠a de caracter√≠sticas\n",
    "\n",
    "**Caso de uso ideal:**\n",
    "- Prototipos r√°pidos\n",
    "- Aplicaciones m√≥viles\n",
    "- Datasets peque√±os\n",
    "- Cuando el tiempo de inferencia es cr√≠tico\n",
    "\n",
    "---\n",
    "\n",
    "### 2. DeepNet (CNN Profunda)\n",
    "\n",
    "**Fortalezas:**\n",
    "- ‚úÖ **Alta capacidad**: Mayor profundidad permite aprender patrones complejos\n",
    "- ‚úÖ **Representaciones ricas**: Jerarqu√≠a de caracter√≠sticas bien definida\n",
    "- ‚úÖ **Performance**: Generalmente mejor accuracy que SimpleNet\n",
    "\n",
    "**Debilidades:**\n",
    "- ‚ùå **Overfitting**: Requiere m√°s regularizaci√≥n y datos\n",
    "- ‚ùå **Costo computacional**: Entrenamiento e inferencia m√°s lentos\n",
    "- ‚ùå **Memoria**: Mayor footprint\n",
    "\n",
    "**Caso de uso ideal:**\n",
    "- Datasets grandes y complejos\n",
    "- Cuando accuracy es prioritario sobre eficiencia\n",
    "- Recursos computacionales disponibles\n",
    "- Producci√≥n con GPUs\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transfer Learning (MobileNetV2)\n",
    "\n",
    "**Fortalezas:**\n",
    "- ‚úÖ **Conocimiento pre-aprendido**: Converge m√°s r√°pido\n",
    "- ‚úÖ **Generalizaci√≥n**: Mejor performance con menos datos\n",
    "- ‚úÖ **SOTA Architecture**: Dise√±o optimizado por expertos\n",
    "- ‚úÖ **Production-ready**: Ampliamente probado en industria\n",
    "\n",
    "**Debilidades:**\n",
    "- ‚ùå **Menos flexible**: Arquitectura fija\n",
    "- ‚ùå **Domain mismatch**: ImageNet ‚Üí CIFAR puede no ser ideal\n",
    "- ‚ùå **Black box**: M√°s dif√≠cil entender internamente\n",
    "\n",
    "**Caso de uso ideal:**\n",
    "- Datasets peque√±os\n",
    "- Desarrollo r√°pido\n",
    "- Baseline de alta calidad\n",
    "- Deployment en producci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "## Mejor Modelo y Justificaci√≥n\n",
    "\n",
    "### üèÜ Ganador: [COMPLETA AQU√ç BASADO EN TUS RESULTADOS]\n",
    "\n",
    "**An√°lisis cuantitativo:**\n",
    "- Test Accuracy: [VALOR]%\n",
    "- Test Loss: [VALOR]\n",
    "- Par√°metros: [CANTIDAD]\n",
    "- Tiempo de entrenamiento: [√âPOCAS]\n",
    "\n",
    "**An√°lisis cualitativo:**\n",
    "- [ANALIZA LAS CURVAS DE APRENDIZAJE]\n",
    "- [DISCUTE OVERFITTING/UNDERFITTING]\n",
    "- [EVAL√öA MATRICES DE CONFUSI√ìN]\n",
    "\n",
    "---\n",
    "\n",
    "## Mejoras Propuestas\n",
    "\n",
    "### Para SimpleNet:\n",
    "1. **Aumentar profundidad selectivamente**: Agregar 1 bloque m√°s en lugar de 3\n",
    "2. **Skip connections**: Implementar residual blocks simples\n",
    "3. **Data augmentation agresivo**: Compensar menor capacidad\n",
    "\n",
    "### Para DeepNet:\n",
    "1. **Residual connections**: Facilitar flujo de gradientes\n",
    "2. **Attention mechanisms**: Focus en regiones importantes\n",
    "3. **Label smoothing**: Reducir overconfidence\n",
    "4. **Mixup/CutMix**: Data augmentation avanzado\n",
    "\n",
    "### Para Transfer Learning:\n",
    "1. **Fine-tuning**: Descongelar √∫ltimas capas de MobileNetV2\n",
    "2. **Progressive unfreezing**: Descongelar gradualmente\n",
    "3. **Discriminative learning rates**: LR diferente por capa\n",
    "4. **Ensembling**: Combinar con arquitecturas propias\n",
    "\n",
    "---\n",
    "\n",
    "## Lecciones Arquitect√≥nicas Aprendidas\n",
    "\n",
    "### Principios validados:\n",
    "1. **Data augmentation es crucial**: Mejora generalizaci√≥n significativamente\n",
    "2. **Regularizaci√≥n multi-fac√©tica**: Dropout + L2 + BatchNorm trabajan bien juntos\n",
    "3. **Transfer learning es poderoso**: Especialmente con datasets peque√±os\n",
    "4. **Trade-off complejidad-performance**: M√°s profundo ‚â† siempre mejor\n",
    "\n",
    "### Consideraciones de producci√≥n:\n",
    "- **Escalabilidad**: SimpleNet escala mejor a m√∫ltiples desarrolladores\n",
    "- **Mantenibilidad**: C√≥digo modular facilita iteraci√≥n\n",
    "- **Deployment**: MobileNetV2 tiene mejor soporte de frameworks\n",
    "- **Monitoring**: Arquitecturas simples m√°s f√°ciles de debuggear\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√≥ximos Pasos\n",
    "\n",
    "### Experimentaci√≥n:\n",
    "1. Probar CIFAR-100 (100 clases)\n",
    "2. Implementar arquitecturas modernas (EfficientNet, Vision Transformer)\n",
    "3. Hyperparameter tuning autom√°tico (Keras Tuner)\n",
    "4. Cross-validation para robustez\n",
    "\n",
    "### Optimizaci√≥n:\n",
    "1. Quantization para deployment m√≥vil\n",
    "2. Pruning para reducir tama√±o\n",
    "3. Knowledge distillation (DeepNet ‚Üí SimpleNet)\n",
    "4. Mixed precision training\n",
    "\n",
    "### Investigaci√≥n:\n",
    "1. ¬øPor qu√© ciertas clases se confunden m√°s?\n",
    "2. An√°lisis de caracter√≠sticas aprendidas (activation maps)\n",
    "3. Adversarial robustness\n",
    "4. Fairness across clases\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias y Recursos\n",
    "\n",
    "**Papers fundamentales:**\n",
    "- VGG: Simonyan & Zisserman (2014)\n",
    "- ResNet: He et al. (2015)\n",
    "- MobileNets: Howard et al. (2017)\n",
    "\n",
    "**Implementaciones de referencia:**\n",
    "- Keras Applications: https://keras.io/api/applications/\n",
    "- TensorFlow Hub: https://tfhub.dev/\n",
    "\n",
    "**Buenas pr√°cticas:**\n",
    "- CS231n Stanford: http://cs231n.stanford.edu/\n",
    "- Deep Learning Book: Goodfellow et al.\n",
    "\n",
    "---\n",
    "\n",
    "*Fin del an√°lisis - Notebook completo y listo para ejecutar*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
